{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Job Shop Problem\n",
    "\n",
    "One common scheduling problem is the job shop, in which multiple jobs are processed on several machines.\n",
    "\n",
    "Each job consists of a sequence of tasks, which must be performed in a given order, and each task must be processed on a specific machine. For example, the job could be the manufacture of a single consumer item, such as an automobile. The problem is to schedule the tasks on the machines so as to minimize the length of the schedule—the time it takes for all the jobs to be completed.\n",
    "\n",
    "There are several constraints for the job shop problem:\n",
    "\n",
    "- No task for a job can be started until the previous task for that job is completed.\n",
    "- A machine can only work on one task at a time.\n",
    "- A task, once started, must run to completion.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ortools\n",
      "  Downloading ortools-9.10.4067-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: absl-py>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from ortools) (2.1.0)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /opt/conda/lib/python3.10/site-packages (from ortools) (1.26.4)\n",
      "Requirement already satisfied: pandas>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from ortools) (2.2.2)\n",
      "Collecting protobuf>=5.26.1 (from ortools)\n",
      "  Downloading protobuf-5.27.0-cp38-abi3-manylinux2014_x86_64.whl.metadata (592 bytes)\n",
      "Collecting immutabledict>=3.0.0 (from ortools)\n",
      "  Downloading immutabledict-4.2.0-py3-none-any.whl.metadata (3.4 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas>=2.0.0->ortools) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=2.0.0->ortools) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas>=2.0.0->ortools) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas>=2.0.0->ortools) (1.16.0)\n",
      "Downloading ortools-9.10.4067-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.7/26.7 MB\u001b[0m \u001b[31m61.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading immutabledict-4.2.0-py3-none-any.whl (4.7 kB)\n",
      "Downloading protobuf-5.27.0-cp38-abi3-manylinux2014_x86_64.whl (309 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m309.2/309.2 kB\u001b[0m \u001b[31m26.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: protobuf, immutabledict, ortools\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 3.20.3\n",
      "    Uninstalling protobuf-3.20.3:\n",
      "      Successfully uninstalled protobuf-3.20.3\n",
      "\u001b[33m  WARNING: Failed to remove contents in a temporary directory '/opt/conda/lib/python3.10/site-packages/google/~rotobuf'.\n",
      "  You can safely remove it manually.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "google-api-core 1.34.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<4.0.0dev,>=3.19.5, but you have protobuf 5.27.0 which is incompatible.\n",
      "google-cloud-aiplatform 1.52.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.27.0 which is incompatible.\n",
      "google-cloud-artifact-registry 1.11.3 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.27.0 which is incompatible.\n",
      "google-cloud-bigquery-storage 2.25.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.27.0 which is incompatible.\n",
      "google-cloud-datastore 1.15.5 requires protobuf<4.0.0dev, but you have protobuf 5.27.0 which is incompatible.\n",
      "google-cloud-language 2.13.3 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.27.0 which is incompatible.\n",
      "google-cloud-monitoring 2.21.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.27.0 which is incompatible.\n",
      "google-cloud-resource-manager 1.12.3 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.27.0 which is incompatible.\n",
      "googleapis-common-protos 1.63.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0.dev0,>=3.19.5, but you have protobuf 5.27.0 which is incompatible.\n",
      "grpc-google-iam-v1 0.13.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.27.0 which is incompatible.\n",
      "kfp 2.5.0 requires protobuf<4,>=3.13.0, but you have protobuf 5.27.0 which is incompatible.\n",
      "kfp-pipeline-spec 0.2.2 requires protobuf<4,>=3.13.0, but you have protobuf 5.27.0 which is incompatible.\n",
      "opentelemetry-proto 1.24.0 requires protobuf<5.0,>=3.19, but you have protobuf 5.27.0 which is incompatible.\n",
      "proto-plus 1.23.0 requires protobuf<5.0.0dev,>=3.19.0, but you have protobuf 5.27.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed immutabledict-4.2.0 ortools-9.10.4067 protobuf-5.27.0\n"
     ]
    }
   ],
   "source": [
    "# !pip install ortools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "from ortools.sat.python import cp_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the data\n",
    "Next, the program defines the data for the problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21\n"
     ]
    }
   ],
   "source": [
    "jobs_data = [  # task = (machine_id, processing_time).\n",
    "    [(0, 3), (1, 2), (2, 2)],  # Job0\n",
    "    [(0, 2), (2, 1), (1, 4)],  # Job1\n",
    "    [(1, 4), (2, 3)],  # Job2\n",
    "]\n",
    "\n",
    "machines_count = 1 + max(task[0] for job in jobs_data for task in job)\n",
    "all_machines = range(machines_count)\n",
    "# Computes horizon dynamically as the sum of all durations.\n",
    "horizon = sum(task[1] for job in jobs_data for task in job)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Declare the model\n",
    "\n",
    "The following code declares the model for the problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each job and task, the program uses the model's `NewIntVar/new_int_var/newIntVar` method to create the variables:\n",
    "\n",
    "- `start_var`: Start time of the task.\n",
    "- `end_var`: End time of the task.\n",
    "The upper bound for start_var and end_var is horizon, the sum of the processing times for all tasks in all jobs. horizon is sufficiently large to complete all tasks for the following reason: if you schedule the tasks - in non-overlapping time intervals (a non-optimal solution), the total length of the schedule is exactly horizon. So the duration of the optimal solution can't be any greater than horizon.\n",
    "\n",
    "Next, the program uses the `NewIntervalVar/new_interval_var/newIntervalVar` method to create an **interval variable** —whose value is a variable time interval — for the task. The inputs to this method are:\n",
    "\n",
    "- The start time of the task.\n",
    "- The length of the time interval for the task.\n",
    "- The end time of the task.\n",
    "- The name for the interval variable.\n",
    "\n",
    "In any solution, `end_var` minus `start_var` must equal `duration`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'list'>, {0: [interval_0_0(start = start_0_0, size = 3, end = end_0_0), interval_1_0(start = start_1_0, size = 2, end = end_1_0)], 1: [interval_0_1(start = start_0_1, size = 2, end = end_0_1), interval_1_2(start = start_1_2, size = 4, end = end_1_2), interval_2_0(start = start_2_0, size = 4, end = end_2_0)], 2: [interval_0_2(start = start_0_2, size = 2, end = end_0_2), interval_1_1(start = start_1_1, size = 1, end = end_1_1), interval_2_1(start = start_2_1, size = 3, end = end_2_1)]})\n"
     ]
    }
   ],
   "source": [
    "model = cp_model.CpModel()\n",
    "\n",
    "# Named tuple to store information about created variables.\n",
    "task_type = collections.namedtuple(\"task_type\", \"start end interval\")\n",
    "# Named tuple to manipulate solution information.\n",
    "assigned_task_type = collections.namedtuple(\n",
    "    \"assigned_task_type\", \"start job index duration\"\n",
    ")\n",
    "\n",
    "# Creates job intervals and add to the corresponding machine lists.\n",
    "all_tasks = {}\n",
    "machine_to_intervals = collections.defaultdict(list)\n",
    "\n",
    "for job_id, job in enumerate(jobs_data):\n",
    "    for task_id, task in enumerate(job):\n",
    "        machine, duration = task\n",
    "        suffix = f\"_{job_id}_{task_id}\"\n",
    "        start_var = model.new_int_var(0, horizon, \"start\" + suffix)\n",
    "        end_var = model.new_int_var(0, horizon, \"end\" + suffix)\n",
    "        interval_var = model.new_interval_var(\n",
    "            start_var, duration, end_var, \"interval\" + suffix\n",
    "        )\n",
    "        all_tasks[job_id, task_id] = task_type(\n",
    "            start=start_var, end=end_var, interval=interval_var\n",
    "        )\n",
    "        machine_to_intervals[machine].append(interval_var)\n",
    "\n",
    "print(machine_to_intervals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the constraints\n",
    "\n",
    "The program uses the model's `AddNoOverlap/add_no_overlap/addNoOverlap` method to create the no overlap constraints, which prevent tasks for the same machine from overlapping in time.\n",
    "\n",
    "Next, the program adds the precedence constraints, which prevent consecutive tasks for the same job from overlapping in time. For each job and each task in the job, a linear constraint is added to specify that the end time of a task to occur before the start time of the next task in the job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create and add disjunctive constraints.\n",
    "for machine in all_machines:\n",
    "    model.add_no_overlap(machine_to_intervals[machine])\n",
    "\n",
    "# Precedences inside a job.\n",
    "for job_id, job in enumerate(jobs_data):\n",
    "    for task_id in range(len(job) - 1):\n",
    "        model.add(\n",
    "            all_tasks[job_id, task_id + 1].start >= all_tasks[job_id, task_id].end\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the objectives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Makespan objective.\n",
    "obj_var = model.new_int_var(0, horizon, \"makespan\")\n",
    "model.add_max_equality(\n",
    "    obj_var,\n",
    "    [all_tasks[job_id, len(job) - 1].end for job_id, job in enumerate(jobs_data)],\n",
    ")\n",
    "model.minimize(obj_var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Invoke the solver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "solver = cp_model.CpSolver()\n",
    "status = solver.solve(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solution:\n",
      "Optimal Schedule Length: 11.0\n",
      "Machine 0: job_1_task_0   job_0_task_0   \n",
      "           [0,2]          [2,5]          \n",
      "Machine 1: job_2_task_0   job_0_task_1   job_1_task_2   \n",
      "           [0,4]          [5,7]          [7,11]         \n",
      "Machine 2: job_1_task_1   job_2_task_1   job_0_task_2   \n",
      "           [2,3]          [4,7]          [7,9]          \n",
      "\n"
     ]
    }
   ],
   "source": [
    "if status == cp_model.OPTIMAL or status == cp_model.FEASIBLE:\n",
    "    print(\"Solution:\")\n",
    "    # Create one list of assigned tasks per machine.\n",
    "    assigned_jobs = collections.defaultdict(list)\n",
    "    for job_id, job in enumerate(jobs_data):\n",
    "        for task_id, task in enumerate(job):\n",
    "            machine = task[0]\n",
    "            assigned_jobs[machine].append(\n",
    "                assigned_task_type(\n",
    "                    start=solver.value(all_tasks[job_id, task_id].start),\n",
    "                    job=job_id,\n",
    "                    index=task_id,\n",
    "                    duration=task[1],\n",
    "                )\n",
    "            )\n",
    "\n",
    "    # Create per machine output lines.\n",
    "    output = \"\"\n",
    "    for machine in all_machines:\n",
    "        # Sort by starting time.\n",
    "        assigned_jobs[machine].sort()\n",
    "        sol_line_tasks = \"Machine \" + str(machine) + \": \"\n",
    "        sol_line = \"           \"\n",
    "\n",
    "        for assigned_task in assigned_jobs[machine]:\n",
    "            name = f\"job_{assigned_task.job}_task_{assigned_task.index}\"\n",
    "            # add spaces to output to align columns.\n",
    "            sol_line_tasks += f\"{name:15}\"\n",
    "\n",
    "            start = assigned_task.start\n",
    "            duration = assigned_task.duration\n",
    "            sol_tmp = f\"[{start},{start + duration}]\"\n",
    "            # add spaces to output to align columns.\n",
    "            sol_line += f\"{sol_tmp:15}\"\n",
    "\n",
    "        sol_line += \"\\n\"\n",
    "        sol_line_tasks += \"\\n\"\n",
    "        output += sol_line_tasks\n",
    "        output += sol_line\n",
    "\n",
    "    # Finally print the solution found.\n",
    "    print(f\"Optimal Schedule Length: {solver.objective_value}\")\n",
    "    print(output)\n",
    "else:\n",
    "    print(\"No solution found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code from copilot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required libraries\n",
    "from ortools.sat.python import cp_model\n",
    "\n",
    "def main():\n",
    "    # Create a CP-SAT solver\n",
    "    model = cp_model.CpModel()\n",
    "\n",
    "    # Define the variables\n",
    "    num_machines = 3\n",
    "    num_manpower = 3\n",
    "    machines = range(num_machines)\n",
    "    manpower = range(num_manpower)\n",
    "\n",
    "    # Create task variables\n",
    "    tasks = {}\n",
    "    for m in machines:\n",
    "        for p in manpower:\n",
    "            tasks[(m, p)] = model.NewBoolVar(f\"Task_{m}_{p}\")\n",
    "\n",
    "    # Each machine can handle only one task\n",
    "    for m in machines:\n",
    "        model.Add(sum(tasks[(m, p)] for p in manpower) == 1)\n",
    "\n",
    "    # Each task requires 1 man and 3 hours on a machine\n",
    "    for p in manpower:\n",
    "        model.Add(sum(tasks[(m, p)] for m in machines) == 1)\n",
    "\n",
    "    # Define the objective function (minimize makespan)\n",
    "    makespan = model.NewIntVar(0, 100, \"Makespan\")\n",
    "    for m in machines:\n",
    "        for p in manpower:\n",
    "            model.Add(makespan >= (m + 1) * tasks[(m, p)])\n",
    "\n",
    "    model.Minimize(makespan)\n",
    "\n",
    "    # Create a solver and solve the problem\n",
    "    solver = cp_model.CpSolver()\n",
    "    status = solver.Solve(model)\n",
    "\n",
    "    if status == cp_model.OPTIMAL:\n",
    "        print(\"Optimal solution found:\")\n",
    "        for m in machines:\n",
    "            for p in manpower:\n",
    "                if solver.Value(tasks[(m, p)]):\n",
    "                    print(f\"Task {p} assigned to Machine {m}\")\n",
    "    else:\n",
    "        print(\"No optimal solution found.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m121",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m121"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
